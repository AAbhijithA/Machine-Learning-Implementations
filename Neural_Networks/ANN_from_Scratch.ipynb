{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "pKEC12fTTuGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "from math import sqrt, exp\n",
        "from numpy.random import randn\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "S4_AgaXX6hA6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the boston dataset (It doesn't work via sklearn so we need to use URL to fetch it)"
      ],
      "metadata": {
        "id": "VihugOPzTxsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "Y = raw_df.values[1::2, 2]"
      ],
      "metadata": {
        "id": "S4I6XUtRbgy5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffling the arrays in Unison"
      ],
      "metadata": {
        "id": "XZvPqCAhT1Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = shuffle(X, Y, random_state = 371)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "BAUdro9cZx2e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function implementations for Neural Network from Scratch"
      ],
      "metadata": {
        "id": "XXo3q759T4Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def He_WeightInit(prevNodes,curNodes):\n",
        "    std = sqrt(2.0/prevNodes)\n",
        "    W = []\n",
        "    for i in range(0,curNodes):\n",
        "        wi = randn(prevNodes)\n",
        "        wi = wi * std\n",
        "        W.append(wi)\n",
        "    W = np.array(W)\n",
        "    return W\n",
        "\n",
        "def LeakyReLU(x):\n",
        "    return max(x,0.01*x);\n",
        "\n",
        "def LeakyReLUderiv(x):\n",
        "    if(x >= 0):\n",
        "        return 1\n",
        "    return 0.01\n",
        "\n",
        "def Identity(x):\n",
        "    return x\n",
        "\n",
        "def IdentityDeriv(x):\n",
        "    return 1\n",
        "\n",
        "def Init_Bias(n):\n",
        "    b = [0.01 for i in range(0,n)]\n",
        "    b = np.array(b)\n",
        "    return b.reshape(-1,1)"
      ],
      "metadata": {
        "id": "nrcSAIW4b7sH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network"
      ],
      "metadata": {
        "id": "HPKpsojBUBgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BuildNeuralNetwork(X, Y, HiddenLayers, OutputLayer, Iterations, LearningRate):\n",
        "    InitialDim = len(X[0])\n",
        "    W = []\n",
        "    W.append([])\n",
        "    B = []\n",
        "    B.append([])\n",
        "    for i in range(0, len(HiddenLayers)):\n",
        "        if (i == 0):\n",
        "            W.append(He_WeightInit(InitialDim, HiddenLayers[i]))\n",
        "        else:\n",
        "            W.append(He_WeightInit(HiddenLayers[i-1], HiddenLayers[i]))\n",
        "        B.append(Init_Bias(HiddenLayers[i]))\n",
        "    W.append(He_WeightInit(HiddenLayers[len(HiddenLayers)-1], OutputLayer))\n",
        "    B.append(Init_Bias(OutputLayer))\n",
        "    lReLU = np.vectorize(LeakyReLU)\n",
        "    lReLUderiv = np.vectorize(LeakyReLUderiv)\n",
        "    Idt = np.vectorize(Identity)\n",
        "    idtderiv = np.vectorize(IdentityDeriv)\n",
        "    scores = []\n",
        "    iteration = []\n",
        "    for iter in range(1, Iterations+1):\n",
        "        ypredict = []\n",
        "        dW = [np.zeros_like(w) for w in W]\n",
        "        dB = [np.zeros_like(b) for b in B]\n",
        "        for dataidx in range(0, len(X)):\n",
        "            # Forward Propagation\n",
        "            x = X[dataidx].reshape(-1, 1)\n",
        "            Z = []\n",
        "            A = []\n",
        "            Z.append(x)\n",
        "            A.append(x)\n",
        "            for layer in range(1, len(W)):\n",
        "                Wi = W[layer]\n",
        "                Zi = np.add(np.dot(Wi, x), B[layer])\n",
        "                Z.append(Zi)\n",
        "                if layer == (len(W) - 1):\n",
        "                    Ai = Idt(Zi)\n",
        "                else:\n",
        "                    Ai = lReLU(Zi)\n",
        "                A.append(Ai)\n",
        "                x = Ai\n",
        "            ypredict.append(x[0])\n",
        "\n",
        "            # Back Propagation\n",
        "            dZ = np.subtract(x, Y[dataidx])\n",
        "            for layer in range(len(W) - 1, 0, -1):\n",
        "                dW[layer] += np.dot(dZ, A[layer - 1].T)\n",
        "                dB[layer] += dZ\n",
        "                if layer > 1:\n",
        "                    dZ = np.dot(W[layer].T, dZ) * lReLUderiv(Z[layer - 1])\n",
        "\n",
        "        # Weight & Bias Adjustment\n",
        "        for layer in range(1, len(W)):\n",
        "            W[layer] -= LearningRate * (dW[layer] / len(X))\n",
        "            B[layer] -= LearningRate * (dB[layer] / len(X))\n",
        "\n",
        "        # Calculating Metrics\n",
        "        ypredict = np.array(ypredict)\n",
        "        scores.append(r2_score(Y, ypredict))\n",
        "        iteration.append(iter)\n",
        "        if (iter % 10) == 0:\n",
        "            print(\"Iteration (\" + str(iter) + \"): Accuracy = \" + str(scores[len(scores)-1]) + \", Loss = \" + str(mean_squared_error(Y, ypredict)))\n",
        "    plt.plot(iteration, scores, color='green', marker='o', linestyle='dashed',linewidth=2, markersize=2)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Score')\n",
        "    return W, B\n"
      ],
      "metadata": {
        "id": "EWyhvHYFji9K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Artificial Neural Network"
      ],
      "metadata": {
        "id": "UkWqcR_6VIOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W, B = BuildNeuralNetwork(X,Y,[9,9],1,200,0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "ZDkSflQBPZ0W",
        "outputId": "7f14cdb3-820a-4927-adff-bbf6143ef863"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration (10): Accuracy = 0.29288184716344823, Loss = 59.69460061242933\n",
            "Iteration (20): Accuracy = 0.4626824510593909, Loss = 45.360108996484975\n",
            "Iteration (30): Accuracy = 0.573393268589157, Loss = 36.01395091893589\n",
            "Iteration (40): Accuracy = 0.6471693444547759, Loss = 29.78580733941675\n",
            "Iteration (50): Accuracy = 0.7005706846826598, Loss = 25.277689899234392\n",
            "Iteration (60): Accuracy = 0.7310296873013056, Loss = 22.706354417208836\n",
            "Iteration (70): Accuracy = 0.7540596556385027, Loss = 20.76217471189211\n",
            "Iteration (80): Accuracy = 0.7657318680153693, Loss = 19.776811723676534\n",
            "Iteration (90): Accuracy = 0.7774668444090129, Loss = 18.78615022502206\n",
            "Iteration (100): Accuracy = 0.7918787719517371, Loss = 17.56950169851046\n",
            "Iteration (110): Accuracy = 0.8025946300667463, Loss = 16.664873712608948\n",
            "Iteration (120): Accuracy = 0.8126707635944244, Loss = 15.814250992432102\n",
            "Iteration (130): Accuracy = 0.8220231182652584, Loss = 15.024729362105246\n",
            "Iteration (140): Accuracy = 0.8295366867972447, Loss = 14.390437241486042\n",
            "Iteration (150): Accuracy = 0.8357124066498082, Loss = 13.869085712587808\n",
            "Iteration (160): Accuracy = 0.8414486216039605, Loss = 13.384836992141906\n",
            "Iteration (170): Accuracy = 0.847013080938569, Loss = 12.915087804865232\n",
            "Iteration (180): Accuracy = 0.852407172381582, Loss = 12.4597209993803\n",
            "Iteration (190): Accuracy = 0.8573786854312584, Loss = 12.040028074302034\n",
            "Iteration (200): Accuracy = 0.861507749381496, Loss = 11.69145432828255\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the Weight and Bias parameters for the network"
      ],
      "metadata": {
        "id": "-rH2PrnZVE0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in range(1,len(W)):\n",
        "    print(\"Weight Matrix from Layer\" + str(layer) + \" to \" + str(layer + 1))\n",
        "    print(W[layer])\n",
        "    print(\"Bias Matrix from Layer\" + str(layer) + \" to \" + str(layer + 1))\n",
        "    print(B[layer])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "C0-UO1mTUezz",
        "outputId": "8df64739-3aaf-480b-f996-1ccaa1223633"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight Matrix from Layer1 to 2\n",
            "[[-0.40572403  0.29107582 -0.51032961 -0.72050197  0.66286717  0.38085139\n",
            "   0.049178   -0.90231726 -0.05707315 -0.19402248  0.01213305  0.14634097\n",
            "  -0.51678175]\n",
            " [ 0.03640964  0.39042442 -0.57471507  0.23898498  0.28560869  0.24562241\n",
            "   0.51020645 -0.23087265 -0.38746958 -0.35158882  0.05823056 -0.29632452\n",
            "  -0.68606505]\n",
            " [-0.71654258 -0.17346749 -0.66622696 -0.60136049  0.35010242 -0.9290112\n",
            "   0.08281708  0.05258277 -0.26080264  0.08438396  0.1545858  -0.05136584\n",
            "   0.6379824 ]\n",
            " [ 0.41961558 -0.11767573  0.53500589 -0.34398314 -0.28067837  0.47941931\n",
            "  -0.69215035  0.08227407 -0.06540495  0.02564195 -0.17602779 -0.51714089\n",
            "   0.1967323 ]\n",
            " [-0.24259372 -0.11357381  0.65787088  0.10220029 -0.24817662 -0.25630464\n",
            "   0.69639272 -0.43723138  1.04121967  0.31339891 -0.33249505 -0.02620747\n",
            "  -1.45308786]\n",
            " [-0.18723604  0.49387246  0.71280858 -0.08452717 -0.56976824  0.6193757\n",
            "   0.1371768   0.17830774  0.72615947 -0.12768967  0.26557017  0.2200591\n",
            "  -0.51937102]\n",
            " [-0.04870295 -0.3942038   0.0746384  -0.05500813  0.57465241 -0.36605933\n",
            "   0.43288164  0.18950899  0.17399938 -0.46916469  0.28420025 -0.20571484\n",
            "   0.52899104]\n",
            " [-0.5235254   0.04487824 -0.47934437 -0.37732581 -0.21144819  0.90072625\n",
            "  -0.79519343  0.17157746  0.37462311 -0.45535874 -0.58409826  0.08514368\n",
            "  -0.63335706]\n",
            " [-0.31124351 -0.50661205  0.1241259   0.54070308 -0.44179246  0.52738394\n",
            "   0.10566873 -0.54742827 -0.35067185 -0.88295333 -0.10871465  0.1511035\n",
            "  -0.01435052]]\n",
            "Bias Matrix from Layer1 to 2\n",
            "[[ 0.00930869]\n",
            " [-0.18648558]\n",
            " [ 0.47492128]\n",
            " [ 0.18210307]\n",
            " [ 0.04108097]\n",
            " [ 0.52407716]\n",
            " [-0.44147929]\n",
            " [ 0.34051981]\n",
            " [ 1.22454082]]\n",
            "Weight Matrix from Layer2 to 3\n",
            "[[ 0.66594565 -0.37276406  0.31576045 -0.22770717 -0.13156996  0.31099644\n",
            "   0.25204704 -0.2114017  -1.03959321]\n",
            " [-0.21420949  0.08937252 -0.06377162  0.33343804 -0.05545737 -0.0880284\n",
            "   0.32930134 -0.51907995 -0.17729724]\n",
            " [ 1.23333329  0.29055176  0.28070634  0.33150033  0.56518147  0.24974267\n",
            "  -0.45766727  0.75341629  0.9080367 ]\n",
            " [-0.66376116  0.05748437  0.12330113 -0.44121553 -0.47055754  1.06475318\n",
            "  -0.17341381 -0.08192884 -0.31615785]\n",
            " [-0.35037177 -0.82475472  0.91605852  0.24017458  1.18117202  0.39501517\n",
            "   0.0623479  -0.67232918  0.30370216]\n",
            " [-0.503147    0.19028308  0.33277861  0.01146715  0.54670889 -0.34538355\n",
            "  -1.19493101 -0.27181273  0.36733792]\n",
            " [ 0.02503306  0.04359841  0.12768732 -0.40572604  1.03588878  0.56795339\n",
            "  -1.28590896  0.80533141  0.57948669]\n",
            " [-0.73863218 -0.10476089 -0.20357829 -0.36980606 -0.00455015 -0.41960651\n",
            "   0.08875636  0.4585984  -1.10424188]\n",
            " [-0.0502209   0.07659118  0.45238548  0.34182993 -0.7030276   0.50210274\n",
            "  -0.13990997  0.8089977   0.40036643]]\n",
            "Bias Matrix from Layer2 to 3\n",
            "[[-0.07066732]\n",
            " [-0.02781687]\n",
            " [ 0.97535502]\n",
            " [-0.00299528]\n",
            " [ 0.7563834 ]\n",
            " [ 0.02532863]\n",
            " [ 0.76564726]\n",
            " [-0.08771202]\n",
            " [ 0.823141  ]]\n",
            "Weight Matrix from Layer3 to 4\n",
            "[[-0.14929338 -0.14696598  1.8309025  -0.03621579  1.62449134  0.39993006\n",
            "   1.41491161 -0.31245529  1.36662923]]\n",
            "Bias Matrix from Layer3 to 4\n",
            "[[1.29401293]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network for same dataset using Tensorflow"
      ],
      "metadata": {
        "id": "ZYKHZA5YUd0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_relu(x):\n",
        "    return tf.nn.leaky_relu(x, alpha=0.01)\n",
        "def BuildTFNN(X, Y, HiddenLayers, OutputLayer, Iterations, LearningRate):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(X.shape[1],)))\n",
        "    for units in HiddenLayers:\n",
        "        model.add(tf.keras.layers.Dense(units, activation=leaky_relu))\n",
        "    model.add(tf.keras.layers.Dense(OutputLayer, activation='linear'))\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=LearningRate)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    history = model.fit(X, Y, epochs=Iterations, batch_size=506, verbose=1)\n",
        "\n",
        "    y_predict = model.predict(X)\n",
        "    scores = r2_score(Y, y_predict)\n",
        "    loss = mean_squared_error(Y, y_predict)\n",
        "    print(\"Accuracy = {:.4f}, Loss = {:.4f}\".format(scores, loss))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "XutD7N4QTK8-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Builing Tensorflow Model"
      ],
      "metadata": {
        "id": "I-Ruan96VZxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NNmodel = BuildTFNN(X,Y,[9,9],1,200,0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UK_2x-3tNJyx",
        "outputId": "9636a783-cb84-41fb-fec7-5f3db162baf5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "1/1 [==============================] - 2s 2s/step - loss: 577.6169\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 568.4734\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 558.6993\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 548.4751\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 537.8061\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 526.7178\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 31ms/step - loss: 515.3412\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 503.7208\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 491.7970\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 479.5712\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 467.0066\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 454.0568\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 440.7002\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 426.9810\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 412.9384\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 398.6183\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 384.0330\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 369.2035\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 354.1527\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 338.8771\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 323.4151\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 307.7948\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 292.0688\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 276.2455\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 260.3649\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 244.4523\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 228.5213\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 212.5882\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 196.7030\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 180.9280\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 165.3687\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 150.1781\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 135.4590\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 121.3626\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 108.0555\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 95.7304\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 84.5549\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 74.7634\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 66.6298\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 60.3200\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 55.8733\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 53.1722\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 52.0093\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 51.9827\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 52.6493\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 53.4968\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 54.1203\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 54.2405\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 53.6913\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 52.4507\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 50.6257\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 48.3737\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 45.8847\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 43.3458\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 40.8876\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 38.6381\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 36.6777\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 35.0314\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 33.7174\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 32.7149\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 31.9619\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 31.4051\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 30ms/step - loss: 30.9916\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 30.6729\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 30.4032\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 30.1485\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 29.8821\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 29.5880\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 29.2539\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 28.8777\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 28.4634\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 28.0185\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 27.5531\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 27.0806\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 26.6143\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 26.1676\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 25.7515\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 25.3744\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 25.0391\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 24.7426\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 24.4800\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 24.2503\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 24.0482\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 23.8581\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 23.6711\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 23.4805\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 23.2798\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 23.0716\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 22.8544\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 22.6286\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 22.3949\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 22.1586\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 21.9246\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 21.6961\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 21.4750\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 21.2635\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 21.0629\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 20.8728\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 20.6932\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 20.5202\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 20.3500\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 20.1828\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 20.0220\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 19.8638\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 19.7079\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 19.5543\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 19.4020\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 19.2509\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 19.1051\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 18.9665\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 18.8316\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 18.7009\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 18.5747\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 18.4519\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 18.3323\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 18.2156\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 18.1016\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 17.9901\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 17.8806\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 17.7726\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 17.6662\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 17.5619\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 17.4594\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 17.3583\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 17.2596\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 17.1630\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 17.0684\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 16.9749\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 16.8836\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 16.7942\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 16.7059\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 16.6182\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 16.5320\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 16.4463\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 16.3615\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 16.2774\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 16.1939\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 16.1108\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 16.0286\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 15.9474\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 15.8673\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 15.7879\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 15.7091\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 15.6309\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 15.5531\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 15.4756\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 15.3976\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 15.3190\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 15.2404\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 15.1623\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 15.0843\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 15.0060\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 14.9284\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 14.8524\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 14.7770\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 14.7018\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 14.6264\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 14.5513\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 14.4768\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 14.4033\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 14.3318\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 14.2615\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 14.1915\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 14.1223\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 14.0529\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13.9813\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 13.9089\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 13.8358\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 13.7630\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13.6909\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13.6185\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13.5464\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13.4749\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 13.4036\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 13.3333\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13.2638\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13.1956\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13.1281\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13.0605\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.9929\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 12.9264\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.8609\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.7963\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 12.7328\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 12.6701\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.6084\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.5468\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 12.4860\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.4254\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.3653\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.3051\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 12.2443\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 12.1826\n",
            "Epoch 194/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.1199\n",
            "Epoch 195/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.0581\n",
            "Epoch 196/200\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 11.9971\n",
            "Epoch 197/200\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 11.9368\n",
            "Epoch 198/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 11.8766\n",
            "Epoch 199/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 11.8170\n",
            "Epoch 200/200\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 11.7579\n",
            "16/16 [==============================] - 0s 3ms/step\n",
            "Accuracy = 0.8614, Loss = 11.6989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Comparison\n",
        "\n",
        "* Both the neural networks achieved similar performance metrics.\n",
        "The neural network built from scratch using NumPy and mathematical libraries achieved an accuracy of 0.8615 and a loss of 11.69.\n",
        "* The neural network built using TensorFlow achieved an accuracy of 0.8614 and a loss of 11.75.\n",
        "* This demonstrates that the custom implementation matches the performance of the TensorFlow-based implementation.\n",
        "\n",
        "## Gradient Descent Techniques:\n",
        "\n",
        "* Both neural networks learned using gradient descent techniques with learning rate of 0.01 and batch size of 506.\n",
        "* This indicates that the optimization algorithms used in both implementations were effective in minimizing the loss function.\n",
        "\n",
        "## Architecture Details:\n",
        "\n",
        "* The architecture of both neural networks included an input layer with 13 nodes, followed by 2 hidden layers.\n",
        "* Leaky ReLU activation function was used in the hidden layers.\n",
        "* Each hidden layer consisted of 9 nodes.\n",
        "* The output layer utilized the Identity activation function.\n",
        "* The use of He Weight Initializer suggests that the weights were initialized appropriately for effective learning for the scratch implementation.\n",
        "\n",
        "## Conclusion:\n",
        "\n",
        "* The successful performance of the custom-built neural network demonstrates an understanding of fundamental concepts in neural network implementation.\n",
        "* Both implementations achieved comparable results, indicating proficiency in both custom implementation and using high-level frameworks like TensorFlow."
      ],
      "metadata": {
        "id": "A3HGxJ3gWWkx"
      }
    }
  ]
}
